1.训练扩展定律

依赖训练数据量、模型规模和计算资源的增加提高大模型性能。随着高质量预训练语料的逐渐枯竭，这种“训练扩展”的方式已接近饱和点，性能提升变得困难。

2.推理扩展

在推理（测试）阶段引入更多计算成本的方法。

2.1 重复采样与选择：例如 best-of-n 或 多数投票，通过多次生成答案并选择最优的一个来提高准确率（Brown et al., 2024b、Wu et al., 2024a）。

2.2 自我修正：LLMs能够根据内部（Madaan et al., 2024）或外部（Jiang et al., 2023b）反馈来优化自身生成的答案（Kamoi et al., 2024、Pan et al., 2024）。

·概率论模型解释自我修正

（1）单个问题在每次自校正迭代中正确概率的变化

```math
P(a_{i,t})
=
P(a_{i,t-1}) P(a_{i,t} \mid a_{i,t-1})
+
[1 - P(a_{i,t-1})]
P(a_{i,t} \mid \neg a_{i,t-1})
```
```math
(2)假设 P(a_{i,t} \mid a_{i,t-1}) = P^{con}_i 和 P(a_{i,t} \mid \neg a_{i,t-1}) = P^{cri}_i 是与轮次 t 无关的常数，代入公式 (1) 并整理：

P(a_{i,t})
= (P^{con}_i - P^{cri}_i) P(a_{i,t-1})
+ P^{cri}_i
\quad 
```
(3)导出通项公式。设 \alpha_i = P^{con}_i - P^{cri}_i。则公式 (2) 变为：
```math
P(a_{i,t}) = \alpha_i P(a_{i,t-1}) + P^{cri}_i
```
展开可得
```math
P(a_{i,t}) = \alpha_i^t \cdot P(a_{i,0} + P^{cri}_i \cdot (1 + \alpha_i + \alpha_i^2 + \dots + \alpha_i^{t-1})
```

求极限可得
<img width="331" height="57" alt="图片" src="https://github.com/user-attachments/assets/ce3d1d4e-ee8c-473d-8324-51e9d460da1e" />，
<img width="339" height="42" alt="图片" src="https://github.com/user-attachments/assets/eb17fae7-427d-48b2-9ee5-0194b2164cd8" />




